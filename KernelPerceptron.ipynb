{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c5120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import optimize\n",
    "import networkx as nx\n",
    "from hashlib import blake2b\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ab0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data-challenge-kernel-methods-2022-2023/training_data.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(\"../data-challenge-kernel-methods-2022-2023/training_labels.pkl\", \"rb\") as f:\n",
    "    train_labels = pickle.load(f)\n",
    "\n",
    "with open(\"../data-challenge-kernel-methods-2022-2023/test_data.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f49ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data, dtype=object)\n",
    "train_labels = np.array(train_labels, dtype=float)\n",
    "test_data = np.array(test_data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d0ad09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_svm = train_labels.copy()\n",
    "train_labels_svm[train_labels_svm == 0] = -1\n",
    "np.unique(train_labels_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69866d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLKernel:\n",
    "    def __init__(self, edge_attr=\"labels\", node_attr=\"labels\", iterations=3):\n",
    "        self.edge_attr = edge_attr\n",
    "        self.node_attr = node_attr\n",
    "        self.n_iter = iterations\n",
    "    \n",
    "    def _hash_label(self, label, digest_size):\n",
    "        return blake2b(label.encode(\"ascii\"), digest_size=digest_size).hexdigest()\n",
    "\n",
    "    def _neighborhood_aggregate(self, G, node, node_labels):\n",
    "        \"\"\"\n",
    "        Compute new labels for given node by aggregating\n",
    "        the labels of each node's neighbors.\n",
    "        \"\"\"\n",
    "        label_list = []\n",
    "        for nbr in G.neighbors(node):\n",
    "            prefix = \"\" if self.edge_attr is None else str(G[node][nbr][self.edge_attr])\n",
    "            label_list.append(prefix + node_labels[nbr])\n",
    "        return node_labels[node] + \"\".join(sorted(label_list))\n",
    "\n",
    "    def weisfeiler_lehman_graph_hash(self, G, digest_size=16):\n",
    "        def weisfeiler_lehman_step(G, labels):\n",
    "            \"\"\"\n",
    "            Apply neighborhood aggregation to each node\n",
    "            in the graph.\n",
    "            Computes a dictionary with labels for each node.\n",
    "            \"\"\"\n",
    "            new_labels = {}\n",
    "            for node in G.nodes():\n",
    "                label = self._neighborhood_aggregate(G, node, labels)\n",
    "                new_labels[node] = self._hash_label(label, digest_size)\n",
    "            return new_labels\n",
    "\n",
    "        # set initial node labels\n",
    "        node_labels = {u: str(dd[self.node_attr]) for u, dd in G.nodes(data=True)}\n",
    "\n",
    "        subgraph_hash_counts = {}\n",
    "        for it in range(self.n_iter):\n",
    "            node_labels = weisfeiler_lehman_step(G, node_labels)\n",
    "            counter = Counter(node_labels.values())\n",
    "            # normalize counter\n",
    "            total = np.sum(list(counter.values()))\n",
    "            for k in counter:\n",
    "                counter[k] /= total\n",
    "\n",
    "            # sort the counter, extend total counts\n",
    "            subgraph_hash_counts[it] = sorted(counter.items(), key=lambda x: x[0])\n",
    "\n",
    "        # return _hash_label(str(tuple(subgraph_hash_counts)), digest_size)\n",
    "        return subgraph_hash_counts\n",
    "    \n",
    "    \n",
    "    def compute_phi(self, Z):\n",
    "        phi_list = []\n",
    "        for g in Z:\n",
    "            phi_list.append(self.weisfeiler_lehman_graph_hash(g))\n",
    "        return phi_list\n",
    "    \n",
    "    def compute_kernel(self, wl1, wl2):\n",
    "        k = 0\n",
    "        for i in range(self.n_iter):\n",
    "            dict1 = dict(wl1[i])\n",
    "            dict2 = dict(wl2[i])\n",
    "            # take scalar product only on common keys\n",
    "            common_keys = set(dict1.keys()).intersection(set(dict2.keys()))\n",
    "            k += np.sum([dict1[c]*dict2[c] for c in common_keys])\n",
    "        return k\n",
    "\n",
    "    def compute_kernel_matrix(self, X, Y):\n",
    "        # Precompute phi to deal only with dot products\n",
    "        phi_X = self.compute_phi(X)\n",
    "        if np.array_equal(X, Y):\n",
    "            print(\"Not computing phi again as X=Y\")\n",
    "            phi_Y = phi_X.copy()\n",
    "        else:\n",
    "            phi_Y = self.compute_phi(Y)\n",
    "        ker = np.zeros((len(X), len(Y)))\n",
    "        count_iter = 0\n",
    "        if len(X) == len(Y):\n",
    "            for i in range(len(X)):\n",
    "                for j in range(i, len(Y)):\n",
    "                    ker[i, j] = self.compute_kernel(phi_X[i], phi_Y[j])\n",
    "                    ker[j, i] = ker[i,j]\n",
    "                count_iter += 1\n",
    "                if count_iter % 100 == 0:\n",
    "                    print(f\"Iteration {count_iter}\")\n",
    "        else:\n",
    "            for (i,j) in itertools.product(range(len(X)), range(len(Y))):\n",
    "                ker[i,j] = self.compute_kernel(phi_X[i], phi_Y[j])\n",
    "        print(\"Kernel computed\")\n",
    "        return ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc55b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPerceptron:\n",
    "    \n",
    "    def __init__(self, kernel_mat, epsilon = 1e-3, n_iter=100):\n",
    "        self.kernel = kernel_mat        \n",
    "        self.norm_f = None\n",
    "        self.alpha = None\n",
    "        self.n_iter = n_iter\n",
    "        self.training_data = None\n",
    "        self.accuracy = 0\n",
    "        self.opt_alpha = None\n",
    "        self.acc_list = []\n",
    "        \n",
    "    def fit(self, X, y, K=None, valX=None, valY=None, K_val=None):\n",
    "       #### You might define here any variable needed for the rest of the code\n",
    "        N = len(y)\n",
    "        if K is None:\n",
    "            K = self.kernel(X, X)\n",
    "        self.alpha = np.zeros(N)\n",
    "        \n",
    "        for it in range(self.n_iter):\n",
    "            y_pred = np.sign(self.alpha.T@K)\n",
    "            for i in range(N):\n",
    "                if y_pred[i] == y[i]:\n",
    "                    continue\n",
    "                self.alpha[i] += y[i]\n",
    "            if valX is not None:\n",
    "                y_pred_val = np.sign(self.alpha.T@K_val)\n",
    "                acc = accuracy_score(y_pred_val, valY)\n",
    "                self.acc_list.append(acc)\n",
    "                if acc > self.accuracy:\n",
    "                    self.accuracy = acc\n",
    "                    self.opt_alpha = self.alpha.copy()\n",
    "                    \n",
    "        self.training_data = X\n",
    "        \n",
    "    \n",
    "    def predict(self, X, K=None):\n",
    "        \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "        if K is None:\n",
    "            K = self.kernel(self.training_data, X)\n",
    "        return np.sign(self.opt_alpha.T@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelLogisticRegression:\n",
    "    def __init__(self, kernel_mat, n_iter=100, regul=None):\n",
    "        self.n_iter = n_iter\n",
    "        self.regul = regul\n",
    "        self.alpha = None\n",
    "        self.kernel = kernel_mat        \n",
    "\n",
    "    \n",
    "    def loss(self, k, y):\n",
    "        prob = 1/(1 + np.exp(-self.alpha.T@k))\n",
    "        loss = 1/len(y)*((prob >= .5) - y)**2\n",
    "        if self.regul is not None:\n",
    "            loss += self.regul*np.linalg.norm(self.alpha)\n",
    "        return pred, loss\n",
    "    \n",
    "    def grad_loss(self, pred, real, k):\n",
    "        back = -2*pred*(pred - real)*k*np.exp(self.alpha.T@k)\n",
    "        if self.regul is not None:\n",
    "            back += 2*self.regul*self.alpha\n",
    "        \n",
    "        return back\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, K=None, batch_size=16):\n",
    "        # Initialize params\n",
    "        self.alpha = np.zeros(len(X))\n",
    "        \n",
    "        # Init K in case\n",
    "        if K is None:\n",
    "            K = self.kernel(X, X)\n",
    "        \n",
    "        for it in range(self.n_iter):\n",
    "            b in range(len(X)//batch_size):\n",
    "                X_batch = X[b*batch_size: (b+1)*batch_size]\n",
    "                y_batch = y[b*batch_size: (b+1)*batch_size]\n",
    "                K_batch = K[:, b*batch_size: (b+1)*batch_size]\n",
    "                pred_batch, loss_batch = self.loss(K_batch ,y_batch)\n",
    "                self.alpha = self.alpha\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a76e07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_mat = WLKernel(iterations=5).compute_kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4d58a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.load(\"../matrices/WL_kernel_train_5it.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "70b42982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4800,), (1200,))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train = np.sort(np.random.choice(np.arange(len(train_data)), size=len(train_data)*80//100, replace=False))\n",
    "idx_val = np.setdiff1d(np.arange(len(train_data)), idx_train)\n",
    "train_data[idx_train].shape, train_data[idx_val].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d47a84bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4800, 1200), (4800, 4800))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_train = K[idx_train, :][:, idx_train]\n",
    "K_val = K[idx_train, :][:, idx_val]\n",
    "K_val.shape, K_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7d649d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelPerceptron(kernel_mat=kernel_mat, n_iter=1000)\n",
    "model.fit(train_data[idx_train], train_labels_svm[idx_train], K_train, train_data[idx_val], train_labels_svm[idx_val], K_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fd9a78db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9075,\n",
       " 0.9075,\n",
       " 0.9075,\n",
       " 0.9058333333333334,\n",
       " 0.8725,\n",
       " 0.9125,\n",
       " 0.8508333333333333,\n",
       " 0.905,\n",
       " 0.8766666666666667,\n",
       " 0.9083333333333333,\n",
       " 0.8241666666666667,\n",
       " 0.905,\n",
       " 0.9233333333333333,\n",
       " 0.8633333333333333,\n",
       " 0.9066666666666666,\n",
       " 0.8408333333333333,\n",
       " 0.905,\n",
       " 0.9008333333333334,\n",
       " 0.9275,\n",
       " 0.8408333333333333,\n",
       " 0.9066666666666666,\n",
       " 0.8891666666666667,\n",
       " 0.9208333333333333,\n",
       " 0.7991666666666667,\n",
       " 0.9058333333333334,\n",
       " 0.9083333333333333,\n",
       " 0.85,\n",
       " 0.9075,\n",
       " 0.8425,\n",
       " 0.9066666666666666,\n",
       " 0.8625,\n",
       " 0.9125,\n",
       " 0.8316666666666667,\n",
       " 0.9058333333333334,\n",
       " 0.91,\n",
       " 0.9166666666666666,\n",
       " 0.905,\n",
       " 0.9225,\n",
       " 0.8583333333333333,\n",
       " 0.9066666666666666,\n",
       " 0.8,\n",
       " 0.9058333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.83,\n",
       " 0.905,\n",
       " 0.8841666666666667,\n",
       " 0.9191666666666667,\n",
       " 0.8158333333333333,\n",
       " 0.9066666666666666,\n",
       " 0.9233333333333333,\n",
       " 0.9025,\n",
       " 0.9241666666666667,\n",
       " 0.8491666666666666,\n",
       " 0.9058333333333334,\n",
       " 0.8408333333333333,\n",
       " 0.9058333333333334,\n",
       " 0.875,\n",
       " 0.9183333333333333,\n",
       " 0.8066666666666666,\n",
       " 0.9066666666666666,\n",
       " 0.9233333333333333,\n",
       " 0.86,\n",
       " 0.9183333333333333,\n",
       " 0.8233333333333334,\n",
       " 0.905,\n",
       " 0.905,\n",
       " 0.92,\n",
       " 0.895,\n",
       " 0.9241666666666667,\n",
       " 0.8666666666666667,\n",
       " 0.9158333333333334,\n",
       " 0.78,\n",
       " 0.9058333333333334,\n",
       " 0.905,\n",
       " 0.8708333333333333,\n",
       " 0.9208333333333333,\n",
       " 0.8683333333333333,\n",
       " 0.9208333333333333,\n",
       " 0.8675,\n",
       " 0.9208333333333333,\n",
       " 0.8625,\n",
       " 0.9166666666666666,\n",
       " 0.8316666666666667,\n",
       " 0.9058333333333334,\n",
       " 0.875,\n",
       " 0.9216666666666666,\n",
       " 0.8741666666666666,\n",
       " 0.92,\n",
       " 0.8808333333333334,\n",
       " 0.92,\n",
       " 0.89,\n",
       " 0.9275,\n",
       " 0.9066666666666666,\n",
       " 0.9175,\n",
       " 0.9091666666666667,\n",
       " 0.915,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9125,\n",
       " 0.915,\n",
       " 0.9141666666666667,\n",
       " 0.9083333333333333,\n",
       " 0.9258333333333333,\n",
       " 0.8983333333333333,\n",
       " 0.92,\n",
       " 0.8916666666666667,\n",
       " 0.9208333333333333,\n",
       " 0.8491666666666666,\n",
       " 0.9058333333333334,\n",
       " 0.7825,\n",
       " 0.9058333333333334,\n",
       " 0.9108333333333334,\n",
       " 0.8458333333333333,\n",
       " 0.915,\n",
       " 0.8358333333333333,\n",
       " 0.9058333333333334,\n",
       " 0.8641666666666666,\n",
       " 0.9191666666666667,\n",
       " 0.87,\n",
       " 0.9225,\n",
       " 0.8825,\n",
       " 0.9241666666666667,\n",
       " 0.905,\n",
       " 0.915,\n",
       " 0.9133333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9108333333333334,\n",
       " 0.9091666666666667,\n",
       " 0.9166666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9158333333333334,\n",
       " 0.9083333333333333,\n",
       " 0.92,\n",
       " 0.91,\n",
       " 0.9216666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9208333333333333,\n",
       " 0.91,\n",
       " 0.9166666666666666,\n",
       " 0.9075,\n",
       " 0.9175,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9066666666666666,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.92,\n",
       " 0.9075,\n",
       " 0.9166666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9191666666666667,\n",
       " 0.9058333333333334,\n",
       " 0.9191666666666667,\n",
       " 0.9091666666666667,\n",
       " 0.92,\n",
       " 0.905,\n",
       " 0.9191666666666667,\n",
       " 0.8983333333333333,\n",
       " 0.92,\n",
       " 0.8916666666666667,\n",
       " 0.92,\n",
       " 0.8866666666666667,\n",
       " 0.9216666666666666,\n",
       " 0.8841666666666667,\n",
       " 0.9175,\n",
       " 0.85,\n",
       " 0.91,\n",
       " 0.79,\n",
       " 0.905,\n",
       " 0.9108333333333334,\n",
       " 0.8516666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.8516666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.8491666666666666,\n",
       " 0.9125,\n",
       " 0.84,\n",
       " 0.9075,\n",
       " 0.8116666666666666,\n",
       " 0.9033333333333333,\n",
       " 0.8925,\n",
       " 0.9158333333333334,\n",
       " 0.9066666666666666,\n",
       " 0.9108333333333334,\n",
       " 0.9075,\n",
       " 0.9083333333333333,\n",
       " 0.9058333333333334,\n",
       " 0.9116666666666666,\n",
       " 0.9066666666666666,\n",
       " 0.9125,\n",
       " 0.9041666666666667,\n",
       " 0.9141666666666667,\n",
       " 0.9083333333333333,\n",
       " 0.9141666666666667,\n",
       " 0.9075,\n",
       " 0.9141666666666667,\n",
       " 0.9058333333333334,\n",
       " 0.91,\n",
       " 0.905,\n",
       " 0.915,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.91,\n",
       " 0.9058333333333334,\n",
       " 0.9141666666666667,\n",
       " 0.9033333333333333,\n",
       " 0.915,\n",
       " 0.91,\n",
       " 0.9091666666666667,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9091666666666667,\n",
       " 0.905,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.91,\n",
       " 0.905,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9108333333333334,\n",
       " 0.9058333333333334,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9108333333333334,\n",
       " 0.9075,\n",
       " 0.9108333333333334,\n",
       " 0.905,\n",
       " 0.9133333333333333,\n",
       " 0.9075,\n",
       " 0.91,\n",
       " 0.9066666666666666,\n",
       " 0.91,\n",
       " 0.9058333333333334,\n",
       " 0.91,\n",
       " 0.9075,\n",
       " 0.91,\n",
       " 0.9066666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.905,\n",
       " 0.91,\n",
       " 0.9025,\n",
       " 0.9141666666666667,\n",
       " 0.9066666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9066666666666666,\n",
       " 0.91,\n",
       " 0.9058333333333334,\n",
       " 0.91,\n",
       " 0.9058333333333334,\n",
       " 0.91,\n",
       " 0.9066666666666666,\n",
       " 0.91,\n",
       " 0.9041666666666667,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.91,\n",
       " 0.905,\n",
       " 0.9083333333333333,\n",
       " 0.9075,\n",
       " 0.9108333333333334,\n",
       " 0.9058333333333334,\n",
       " 0.9116666666666666,\n",
       " 0.9058333333333334,\n",
       " 0.9116666666666666,\n",
       " 0.9058333333333334,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9066666666666666,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.905,\n",
       " 0.91,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9058333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9058333333333334,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9058333333333334,\n",
       " 0.9141666666666667,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9066666666666666,\n",
       " 0.9116666666666666,\n",
       " 0.9058333333333334,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.905,\n",
       " 0.9091666666666667,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.905,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9066666666666666,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9058333333333334,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9066666666666666,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9108333333333334,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9066666666666666,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9075,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9141666666666667,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9141666666666667,\n",
       " 0.9083333333333333,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9075,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9141666666666667,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9125,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9141666666666667,\n",
       " 0.9116666666666666,\n",
       " 0.9125,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9108333333333334,\n",
       " 0.9125,\n",
       " 0.9083333333333333,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9116666666666666,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.91,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9091666666666667,\n",
       " 0.9133333333333333,\n",
       " 0.9116666666666666,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9133333333333333,\n",
       " 0.9083333333333333,\n",
       " 0.9133333333333333]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cc514e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1656"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.opt_alpha != model.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0fbb8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = model.predict(train_data[idx_val], K_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9c065899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  0.,  1.]), array([1132,    6,   62]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(val, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3b7209f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.95      0.98      0.96      1095\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       0.69      0.41      0.51       105\n",
      "\n",
      "    accuracy                           0.93      1200\n",
      "   macro avg       0.55      0.46      0.49      1200\n",
      "weighted avg       0.92      0.93      0.92      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebouhanna/Documents/Pro/MVA/env_mva_global/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ebouhanna/Documents/Pro/MVA/env_mva_global/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ebouhanna/Documents/Pro/MVA/env_mva_global/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(train_labels_svm[idx_val], val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e4a911b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 6000)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_test = np.load('../matrices/WL_kernel_test_10it.npy')\n",
    "K_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "62a79f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(test_data, K_test.T[idx_train, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6f94cd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  0.,  1.]), array([1832,    5,  163]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0fcc86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = test.copy()\n",
    "test_preds[test_preds == -1] = 0\n",
    "Yte = {'Predicted' : test_preds}\n",
    "dataframe = pd.DataFrame(Yte) \n",
    "dataframe.index += 1 \n",
    "dataframe.to_csv('test_pred.csv',index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "53502f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Predicted\n",
       "1           0.0\n",
       "2           0.0\n",
       "3           0.0\n",
       "4           0.0\n",
       "5           1.0\n",
       "...         ...\n",
       "1996        0.0\n",
       "1997        0.0\n",
       "1998        0.0\n",
       "1999        1.0\n",
       "2000        0.0\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e236f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
