{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba737137",
   "metadata": {},
   "source": [
    "# Implementing Support Vector Machines with graph kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a106ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import optimize\n",
    "import networkx as nx\n",
    "from hashlib import blake2b\n",
    "from collections import Counter, defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "429b8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data-challenge-kernel-methods-2022-2023/training_data.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(\"./data-challenge-kernel-methods-2022-2023/training_labels.pkl\", \"rb\") as f:\n",
    "    train_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "15875b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_svm = train_labels.copy()\n",
    "train_labels_svm[train_labels_svm == 0] = -1\n",
    "np.unique(train_labels_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9ecea666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSVC:\n",
    "    \n",
    "    def __init__(self, C, kernel_mat, epsilon = 1e-3):\n",
    "        self.type = 'non-linear'\n",
    "        self.C = C                               \n",
    "        self.kernel = kernel_mat        \n",
    "        self.alpha = None\n",
    "        self.support = None\n",
    "        self.epsilon = epsilon\n",
    "        self.norm_f = None\n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "       #### You might define here any variable needed for the rest of the code\n",
    "        N = len(y)\n",
    "        K = self.kernel(X, X)\n",
    "        \n",
    "        # Store training data\n",
    "        self.X_train = X\n",
    "        self.obs_train = y\n",
    "\n",
    "        # define matrix form of inequality constraints on dual problem\n",
    "        ineq_A = np.vstack([np.eye(N), -np.eye(N)])\n",
    "        ineq_b = np.array([self.C]*N + [0]*N)\n",
    "\n",
    "        # Lagrange dual problem\n",
    "        def loss(alpha):\n",
    "            # The dual problem is a max pb => max f = min -f\n",
    "            return .5*(alpha*y).T@K@(alpha*y) - alpha.sum()\n",
    "\n",
    "        # Partial derivate of Ld on alpha\n",
    "        def grad_loss(alpha):\n",
    "            return y*((alpha*y)@K) - 1\n",
    "\n",
    "\n",
    "        # Constraints on alpha of the shape :\n",
    "        # -  d - C*alpha  = 0\n",
    "        # -  b - A*alpha >= 0\n",
    "\n",
    "        fun_eq = lambda alpha: alpha@y\n",
    "        jac_eq = lambda alpha: y\n",
    "        fun_ineq = lambda alpha: ineq_b - ineq_A@alpha\n",
    "        jac_ineq = lambda alpha: -ineq_A\n",
    "        \n",
    "        constraints = ({'type': 'eq',  'fun': fun_eq, 'jac': jac_eq},\n",
    "                       {'type': 'ineq', \n",
    "                        'fun': fun_ineq , \n",
    "                        'jac': jac_ineq})\n",
    "\n",
    "        optRes = optimize.minimize(fun=lambda alpha: loss(alpha),\n",
    "                                   x0=np.ones(N), \n",
    "                                   method='SLSQP', \n",
    "                                   jac=lambda alpha: grad_loss(alpha), \n",
    "                                   constraints=constraints)\n",
    "        self.alpha = optRes.x\n",
    "\n",
    "        ## Assign the required attributes\n",
    "        margin_idx = (self.alpha >= self.epsilon)\n",
    "        sv_idx = (self.alpha >= self.epsilon) * (self.C - self.alpha >= self.epsilon)\n",
    "        self.support = X[sv_idx]\n",
    "        self.margin_points = X[margin_idx]\n",
    "        self.margin_obs = y[margin_idx]\n",
    "        self.margin_alpha = self.alpha[margin_idx]\n",
    "        self.support_obs = y[sv_idx]\n",
    "        self.support_alpha = self.alpha[sv_idx]\n",
    "        \n",
    "        # Take mean of offsets for more robust approximation\n",
    "        self.b = np.mean(y[sv_idx] - self.separating_function(self.support))\n",
    "        self.norm_f = np.sqrt(self.margin_alpha@self.kernel(self.margin_points, self.margin_points)@self.margin_alpha)\n",
    "\n",
    "\n",
    "    ### Implementation of the separting function $f$ \n",
    "    def separating_function(self,x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        \n",
    "        return (self.support_alpha*self.support_obs).T@self.kernel(self.support, x)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "        d = self.separating_function(X)\n",
    "        return 2*(d+self.b> 0) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6783d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalkKernel:\n",
    "    def __init__(self, l_walk, n_walks):\n",
    "        self.l = l_walk\n",
    "        self.n_walks = n_walks\n",
    "    \n",
    "    def perform_random_walk(self, edge_list, g, rdm_node):\n",
    "        curr_node = rdm_node\n",
    "        s = ''\n",
    "        for _ in range(self.l + 1):\n",
    "            s += str(g.nodes[curr_node]['labels'][0])\n",
    "            # Undirected graph !\n",
    "            sample_neighbor = edge_list.loc[(edge_list['source'] == curr_node) + (edge_list[\"target\"] == curr_node), :]\n",
    "            if len(sample_neighbor) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sample_neighbor = sample_neighbor.sample()\n",
    "            # Get new node\n",
    "            if sample_neighbor[\"source\"].values == curr_node:\n",
    "                next_node = sample_neighbor[\"target\"].values[0]\n",
    "            else:\n",
    "                next_node = sample_neighbor[\"source\"].values[0]\n",
    "\n",
    "            s+= str(sample_neighbor[\"labels\"].values[0][0])\n",
    "            curr_node = next_node  \n",
    "        return s\n",
    "        \n",
    "    def compute_kernel(self, g0, g1):\n",
    "        \n",
    "        # Store edge list\n",
    "        edges_g0 = nx.to_pandas_edgelist(g0)\n",
    "        edges_g1 = nx.to_pandas_edgelist(g1)\n",
    "\n",
    "        # Run random walks\n",
    "        seq_g0 = []\n",
    "        seq_g1 = []\n",
    "        for _ in range(self.n_walks):\n",
    "            rdm_node_g0 = np.random.choice(g0.nodes)\n",
    "            seq_g0.append(self.perform_random_walk(edges_g0, g0, rdm_node_g0))\n",
    "\n",
    "            rdm_node_g1 = np.random.choice(g1.nodes)\n",
    "            seq_g1.append(self.perform_random_walk(edges_g1, g1, rdm_node_g1))\n",
    "\n",
    "        # Compute kernel\n",
    "        keys, vals = np.unique(seq_g1, return_counts=True)\n",
    "        dico_g1 = dict(zip(list(keys), list(vals)))\n",
    "\n",
    "        keys, vals = np.unique(seq_g0, return_counts=True)\n",
    "        dico_g0 = dict(zip(list(keys), list(vals)))\n",
    "        common_occ = 0\n",
    "        for good_seq in np.intersect1d(seq_g0, seq_g1):\n",
    "            common_occ += min(int(dico_g0[good_seq]), int(dico_g1[good_seq]))\n",
    "        \n",
    "        common_occ /= self.n_walks\n",
    "        \n",
    "        return common_occ\n",
    "    \n",
    "    def compute_kernel_matrix(self, X, Y):\n",
    "        k = np.zeros((len(X), len(Y)))\n",
    "        for i in range(len(X)):\n",
    "            for j in range(i, len(Y)):\n",
    "                k[i, j] = self.compute_kernel(X[i], Y[j]) \n",
    "                k[j, i] = k[i,j]\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2d12dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLKernel:\n",
    "    def __init__(self, edge_attr=\"labels\", node_attr=\"labels\", iterations=3):\n",
    "        self.edge_attr = edge_attr\n",
    "        self.node_attr = node_attr\n",
    "        self.n_iter = iterations\n",
    "    \n",
    "    def _hash_label(self, label, digest_size):\n",
    "        return blake2b(label.encode(\"ascii\"), digest_size=digest_size).hexdigest()\n",
    "\n",
    "    def _neighborhood_aggregate(self, G, node, node_labels):\n",
    "        \"\"\"\n",
    "        Compute new labels for given node by aggregating\n",
    "        the labels of each node's neighbors.\n",
    "        \"\"\"\n",
    "        label_list = []\n",
    "        for nbr in G.neighbors(node):\n",
    "            prefix = \"\" if self.edge_attr is None else str(G[node][nbr][self.edge_attr])\n",
    "            label_list.append(prefix + node_labels[nbr])\n",
    "        return node_labels[node] + \"\".join(sorted(label_list))\n",
    "\n",
    "    def weisfeiler_lehman_graph_hash(self, G, digest_size=16):\n",
    "        def weisfeiler_lehman_step(G, labels):\n",
    "            \"\"\"\n",
    "            Apply neighborhood aggregation to each node\n",
    "            in the graph.\n",
    "            Computes a dictionary with labels for each node.\n",
    "            \"\"\"\n",
    "            new_labels = {}\n",
    "            for node in G.nodes():\n",
    "                label = self._neighborhood_aggregate(G, node, labels)\n",
    "                new_labels[node] = self._hash_label(label, digest_size)\n",
    "            return new_labels\n",
    "\n",
    "        # set initial node labels\n",
    "        node_labels = {u: str(dd[self.node_attr]) for u, dd in G.nodes(data=True)}\n",
    "\n",
    "        subgraph_hash_counts = {}\n",
    "        for it in range(self.n_iter):\n",
    "            node_labels = weisfeiler_lehman_step(G, node_labels)\n",
    "            counter = Counter(node_labels.values())\n",
    "            # normalize counter\n",
    "            total = np.sum(list(counter.values()))\n",
    "            for k in counter:\n",
    "                counter[k] /= total\n",
    "\n",
    "            # sort the counter, extend total counts\n",
    "            subgraph_hash_counts[it] = sorted(counter.items(), key=lambda x: x[0])\n",
    "\n",
    "        # return _hash_label(str(tuple(subgraph_hash_counts)), digest_size)\n",
    "        return subgraph_hash_counts\n",
    "    \n",
    "    \n",
    "    def compute_phi(self, Z):\n",
    "        phi_list = []\n",
    "        for g in Z:\n",
    "            phi_list.append(self.weisfeiler_lehman_graph_hash(g))\n",
    "        return phi_list\n",
    "    \n",
    "    def compute_kernel(self, wl1, wl2):\n",
    "        k = 0\n",
    "        for i in range(self.n_iter):\n",
    "            dict1 = dict(wl1[i])\n",
    "            dict2 = dict(wl2[i])\n",
    "            # take scalar product only on common keys\n",
    "            common_keys = set(dict1.keys()).intersection(set(dict2.keys()))\n",
    "            k += np.sum([dict1[c]*dict2[c] for c in common_keys])\n",
    "        return k\n",
    "\n",
    "    def compute_kernel_matrix(self, X, Y):\n",
    "        # Precompute phi to deal only with dot products\n",
    "        phi_X = self.compute_phi(X)\n",
    "        if np.array_equal(X, Y):\n",
    "            print(\"Not computing phi again as X=Y\")\n",
    "            phi_Y = phi_X.copy()\n",
    "        else:\n",
    "            phi_Y = self.compute_phi(Y)\n",
    "        ker = np.zeros((len(X), len(Y)))\n",
    "        count_iter = 0\n",
    "        if len(X) == len(Y):\n",
    "            for i in range(len(X)):\n",
    "                for j in range(i, len(Y)):\n",
    "                    ker[i, j] = self.compute_kernel(phi_X[i], phi_Y[j])\n",
    "                    ker[j, i] = ker[i,j]\n",
    "                count_iter += 1\n",
    "                if count_iter % 100 == 0:\n",
    "                    print(f\"Iteration {count_iter}\")\n",
    "        else:\n",
    "            for (i,j) in itertools.product(range(len(X)), range(len(Y))):\n",
    "                ker[i,j] = self.compute_kernel(phi_X[i], phi_Y[j])\n",
    "        print(\"Kernel computed\")\n",
    "        return ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not computing phi again as X=Y\n",
      "Iteration 100\n",
      "Iteration 200\n",
      "Iteration 300\n",
      "Iteration 400\n",
      "Iteration 500\n",
      "Iteration 600\n",
      "Iteration 700\n",
      "Iteration 800\n",
      "Iteration 900\n",
      "Iteration 1000\n",
      "Iteration 1100\n",
      "Iteration 1200\n",
      "Iteration 1300\n",
      "Iteration 1400\n",
      "Iteration 1500\n",
      "Iteration 1600\n",
      "Iteration 1700\n",
      "Iteration 1800\n",
      "Iteration 1900\n",
      "Iteration 2000\n",
      "Iteration 2100\n",
      "Iteration 2200\n",
      "Iteration 2300\n",
      "Iteration 2400\n",
      "Iteration 2500\n",
      "Iteration 2600\n",
      "Iteration 2700\n",
      "Iteration 2800\n",
      "Iteration 2900\n",
      "Iteration 3000\n",
      "Iteration 3100\n",
      "Iteration 3200\n",
      "Iteration 3300\n",
      "Iteration 3400\n",
      "Iteration 3500\n",
      "Iteration 3600\n",
      "Iteration 3700\n",
      "Iteration 3800\n",
      "Iteration 3900\n",
      "Iteration 4000\n",
      "Iteration 4100\n",
      "Iteration 4200\n",
      "Iteration 4300\n",
      "Iteration 4400\n",
      "Iteration 4500\n",
      "Iteration 4600\n",
      "Iteration 4700\n",
      "Iteration 4800\n",
      "Iteration 4900\n",
      "Iteration 5000\n",
      "Kernel computed\n"
     ]
    }
   ],
   "source": [
    "C = 100\n",
    "kernel = WLKernel(iterations=5).compute_kernel_matrix\n",
    "model = KernelSVC(C=C, kernel_mat=kernel)\n",
    "model.fit(np.array(train_data[:5000], dtype=object), np.array(train_labels_svm[:5000], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(np.array(train_data[5000:], dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(train_labels_svm[5000], test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4ed96",
   "metadata": {},
   "source": [
    "## Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original labels are in {0, 1}\n",
    "test_preds = test.copy()\n",
    "test_preds[test_preds == -1] = 0\n",
    "Yte = {'Prediction' : test_preds}\n",
    "dataframe = pd.DataFrame(Yte) \n",
    "dataframe.index += 1 \n",
    "dataframe.to_csv('test_pred.csv',index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
